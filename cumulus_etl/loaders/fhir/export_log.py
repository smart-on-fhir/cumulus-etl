"""
Parsing for bulk export log files

https://github.com/smart-on-fhir/bulk-data-client/wiki/Bulk-Data-Export-Log-Items
"""

import datetime
import json
import os
import re

from cumulus_etl import common, fhir, store


class BulkExportLogParser:
    """
    Parses the log file generated by bulk exports.

    These are the assumptions we make:
    - There will be a log.ndjson or log.*.ndjson file in the given folder.
      - There cannot be multiples (unless log.ndjson exists, in which case we always use that)
    - That log file will be for a single export.
      - e.g. We will generally grab the last "kickoff" event and ignore others.
    """

    class LogParsingError(Exception):
        pass

    class IncompleteLog(LogParsingError):
        pass

    class MultipleLogs(LogParsingError):
        pass

    class NoLogs(LogParsingError):
        pass

    def __init__(self, root: store.Root):
        self.group_name: str = None
        self.export_datetime: datetime.datetime = None

        self._parse(self._find(root))

    def _parse(self, path: str) -> None:
        # Go through every row, looking for the events we care about.
        # Note that we parse every kickoff event we hit, for example.
        # So we'll end up with the latest one (which works for single-export
        # log files with maybe a false start at the beginning).
        try:
            for row in common.read_ndjson(path):
                match row.get("eventId"):
                    case "kickoff":
                        self._parse_kickoff(row)
                    case "status_complete":
                        self._parse_status_complete(row)
        except (KeyError, json.JSONDecodeError) as exc:
            raise self.IncompleteLog(f"Error parsing '{path}'") from exc

        if self.group_name is None:
            raise self.IncompleteLog(f"No kickoff event found in '{path}'")
        if self.export_datetime is None:
            raise self.IncompleteLog(f"No status_complete event found in '{path}'")

    def _parse_kickoff(self, row: dict) -> None:
        details = row["eventDetail"]
        self.group_name = fhir.parse_group_from_url(details["exportUrl"])

    def _parse_status_complete(self, row: dict) -> None:
        details = row["eventDetail"]
        self.export_datetime = datetime.datetime.fromisoformat(details["transactionTime"])

    def _find(self, root: store.Root) -> str:
        """Finds the log file inside the root"""
        try:
            paths = root.ls()
        except FileNotFoundError as exc:
            raise self.NoLogs("Folder does not exist") from exc
        filenames = {os.path.basename(p): p for p in paths}

        # In the easy case, it's just sitting there at log.ndjson,
        # which is the filename that bulk-data-client uses.
        # Because this is the standard name, we prefer this and don't
        # error out even if there are other log.something.ndjson names in
        # the folder (see below). Maybe this is a symlink to the most recent...
        if full_path := filenames.get("log.ndjson"):
            return full_path

        # But possibly the user does some file renaming to manage different
        # exports, so allow log.something.ndjson as well. (Much like we do
        # for the input ndjson files.)
        pattern = re.compile(r"log\..+\.ndjson")
        log_files = list(filter(pattern.match, filenames.keys()))
        match len(log_files):
            case 0:
                raise self.NoLogs("No log.ndjson file found")
            case 1:
                return filenames[log_files[0]]
            case _:
                raise self.MultipleLogs("Multiple log.*.ndjson files found")
