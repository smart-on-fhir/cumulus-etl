"""
Parsing for bulk export log files

https://github.com/smart-on-fhir/bulk-data-client/wiki/Bulk-Data-Export-Log-Items
"""

import datetime
import os
import re

from cumulus_etl import common, fhir, store


class BulkExportLogParser:
    """
    Parses the log file generated by bulk exports.

    These are the assumptions we make:
    - There will be a log.ndjson or log.*.ndjson file in the given folder.
      - There cannot be multiples (unless log.ndjson exists, in which case we always use that)
    - That log file will be for a single export.
      - e.g. We will generally grab the last "kickoff" event and ignore others.
    """

    class LogParsingError(Exception):
        pass

    class IncompleteLog(LogParsingError):
        pass

    class MultipleLogs(LogParsingError):
        pass

    class NoLogs(LogParsingError):
        pass

    def __init__(self, root: store.Root):
        self.group_name: str = None
        self.export_datetime: datetime.datetime = None
        self.export_url: str = None

        self._parse(root, self._find(root))

    def _parse(self, root: store.Root, path: str) -> None:
        # Go through every row, looking for the final kickoff event.
        # We only want to look at one series of events for one bulk export.
        # So we pick the last one, in case there are multiple in the log.
        # Those early events might be false starts.
        export_id = None
        for row in common.read_ndjson(root, path):
            if row.get("eventId") == "kickoff":
                export_id = row.get("exportId")
        if not export_id:
            raise self.IncompleteLog(f"No kickoff event found in '{path}'")

        # Now read through the log file again, only looking for the events from the one export.
        try:
            for row in common.read_ndjson(root, path):
                if row.get("exportId") != export_id:
                    continue
                match row.get("eventId"):
                    case "kickoff":
                        self._parse_kickoff(row)
                    case "status_complete":
                        self._parse_status_complete(row)
        except KeyError as exc:
            raise self.IncompleteLog(f"Error parsing '{path}'") from exc

        if self.export_datetime is None:
            raise self.IncompleteLog(f"No status_complete event found in '{path}'")

    def _parse_kickoff(self, row: dict) -> None:
        details = row["eventDetail"]
        self.group_name = fhir.FhirUrl(details["exportUrl"]).group
        self.export_url = details["exportUrl"]

    def _parse_status_complete(self, row: dict) -> None:
        details = row["eventDetail"]
        self.export_datetime = datetime.datetime.fromisoformat(details["transactionTime"])

    def _find(self, root: store.Root) -> str:
        """Finds the log file inside the root"""
        try:
            paths = root.ls()
        except FileNotFoundError as exc:
            raise self.NoLogs("Folder does not exist") from exc
        filenames = {os.path.basename(p): p for p in paths}

        # In the easy case, it's just sitting there at log.ndjson,
        # which is the filename that bulk-data-client uses.
        # Because this is the standard name, we prefer this and don't
        # error out even if there are other log.something.ndjson names in
        # the folder (see below). Maybe this is a symlink to the most recent...
        if full_path := filenames.get("log.ndjson"):
            return full_path

        # But possibly the user does some file renaming to manage different
        # exports, so allow log.something.ndjson as well. (Much like we do
        # for the input ndjson files.)
        pattern = re.compile(r"log\..+\.ndjson")
        log_files = list(filter(pattern.match, filenames.keys()))
        match len(log_files):
            case 0:
                raise self.NoLogs("No log.ndjson file found")
            case 1:
                return filenames[log_files[0]]
            case _:
                raise self.MultipleLogs("Multiple log.*.ndjson files found")
