"""Load, transform, and write out input data to deidentified FHIR"""

import argparse
import datetime
import os

import cumulus_etl
from cumulus_etl import cli_utils, common, errors, loaders
from cumulus_etl.etl import context, pipeline
from cumulus_etl.etl.config import JobConfig


def define_etl_parser(parser: argparse.ArgumentParser) -> None:
    """Fills out an argument parser with all the ETL options."""
    parser.usage = "%(prog)s [OPTION]... INPUT OUTPUT PHI"

    parser.add_argument("dir_input", metavar="/path/to/input")
    parser.add_argument("dir_output", metavar="/path/to/output")
    parser.add_argument("dir_phi", metavar="/path/to/phi")

    pipeline.add_common_etl_args(parser, outputs=["deltalake", "ndjson"])
    parser.add_argument("--comment", help="add the comment to the log file")
    parser.add_argument(
        "--version", action="version", version=f"cumulus-etl {cumulus_etl.__version__}"
    )
    parser.add_argument(
        "--philter", action="store_true", help="run philter on all freeform text fields"
    )
    parser.add_argument(
        "--allow-missing-resources",
        action="store_true",
        help="run tasks even if their resources are not present",
    )
    cli_utils.add_task_selection(parser)

    cli_utils.add_aws(parser)

    # Mark some option flags as removed
    cli_utils.RemovedEhrArg.add(parser, "--since")
    cli_utils.RemovedEhrArg.add(parser, "--until")
    cli_utils.RemovedEhrArg.add(parser, "--resume")
    cli_utils.RemovedEhrArg.add(parser, "--export-to")

    group = parser.add_argument_group("external export identification")
    group.add_argument(
        "--export-group",
        metavar="NAME",
        help="name of the FHIR Group that was exported (default is to grab this from an "
        "export log file in the input folder, but you can also use this to assign a "
        "nickname as long as you consistently set the same nickname)",
    )
    group.add_argument(
        "--export-timestamp",
        metavar="TIMESTAMP",
        help="when the data was exported from the FHIR Group (default is to grab this from an "
        "export log file in the input folder)",
    )


def handle_completion_args(
    args: argparse.Namespace, loader_results: loaders.LoaderResults
) -> (str, datetime.datetime):
    """Returns (group_name, datetime)"""
    # Grab completion options from CLI or loader
    export_group_name = args.export_group or loader_results.group_name
    export_datetime = (
        datetime.datetime.fromisoformat(args.export_timestamp)
        if args.export_timestamp
        else loader_results.export_datetime
    )

    # Error out if we have missing args
    missing_group_name = export_group_name is None
    missing_datetime = not export_datetime
    if missing_group_name and missing_datetime:
        errors.fatal(
            "Missing Group name and timestamp export information for the input data.",
            errors.COMPLETION_ARG_MISSING,
            details="This is likely because you donâ€™t have an export log in your input folder.\n"
            "This log file (log.ndjson) is generated by some bulk export tools.\n"
            "Instead, please manually specify the Group name and timestamp of the export "
            "with the --export-group and --export-timestamp options.\n"
            "These options are necessary to track whether all the required data from "
            "a Group has been imported and is ready to be used.\n"
            "See https://docs.smarthealthit.org/cumulus/etl/bulk-exports.html for more "
            "information.\n",
        )
    # These next two errors can be briefer because the user clearly knows about the args.
    elif missing_datetime:
        errors.fatal("Missing --export-datetime argument.", errors.COMPLETION_ARG_MISSING)
    elif missing_group_name:
        errors.fatal("Missing --export-group argument.", errors.COMPLETION_ARG_MISSING)

    return export_group_name, export_datetime


async def etl_main(args: argparse.Namespace) -> None:
    job_datetime = common.datetime_now()
    scrubber, loader_results, selected_tasks = await pipeline.prepare_pipeline(
        args, job_datetime, nlp=False
    )

    job_context = context.JobContext(os.path.join(args.dir_phi, "context.json"))

    # Establish the group name and datetime of the loaded dataset (from CLI args or Loader)
    export_group, export_datetime = handle_completion_args(args, loader_results)

    # Prepare config for jobs
    config = JobConfig(
        args.dir_input,
        loader_results.path,
        args.dir_output,
        args.dir_phi,
        args.input_format,
        args.output_format,
        codebook_id=scrubber.codebook.get_codebook_id(),
        comment=args.comment,
        batch_size=args.batch_size,
        timestamp=job_datetime,
        dir_errors=args.errors_to,
        tasks=[t.name for t in selected_tasks],
        export_url=loader_results.export_url,
        deleted_ids=loader_results.deleted_ids,
        export_group_name=export_group,
        export_datetime=export_datetime,
    )
    common.write_json(config.path_config(), config.as_json(), indent=4)

    # Finally, actually run the meat of the pipeline! (Filtered down to requested tasks)
    summaries = await pipeline.etl_job(config, selected_tasks, scrubber)

    # Update job context for future runs
    job_context.last_successful_datetime = job_datetime
    job_context.last_successful_input_dir = args.dir_input
    job_context.last_successful_output_dir = args.dir_output
    job_context.save()

    # Report out any stripped extensions or dropped resources due to modiferExtensions
    scrubber.print_extension_report()

    pipeline.print_summary(summaries)


async def run_etl(parser: argparse.ArgumentParser, argv: list[str]) -> None:
    """Parses an etl CLI"""
    define_etl_parser(parser)
    args = parser.parse_args(argv)
    await etl_main(args)
