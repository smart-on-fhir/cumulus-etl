services:

  cumulus-etl-base:
    image: smartonfhir/cumulus-etl:latest
    build:
      context: .
      target: cumulus-etl
    environment:
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - AWS_SESSION_TOKEN
      - AWS_PROFILE
      - AWS_DEFAULT_PROFILE
      - CUMULUS_HUGGING_FACE_URL
      - URL_CTAKES_REST=http://ctakes-covid:8080/ctakes-web-rest/service/analyze
      - URL_CNLP_NEGATION=http://cnlp-transformers:8000/negation/process
    volumes:
      - $HOME/.aws/:/root/.aws/:ro
      - ctakes-overrides:/ctakes-overrides
    networks:
      - cumulus-etl
    profiles:
      - base

  cumulus-etl:
    extends: cumulus-etl-base
    profiles:
      - etl

  cumulus-etl-gpu:
    extends: cumulus-etl-base
    environment:
      - URL_CNLP_NEGATION=http://cnlp-transformers-gpu:8000/negation/process
    profiles:
      - etl-gpu

  ctakes-covid-base:
    image: smartonfhir/ctakes-covid:1.1
    environment:
      - ctakes_umlsuser=umls_api_key 
      - ctakes_umlspw=$UMLS_API_KEY
    networks:
      - cumulus-etl
    profiles:
      - base
    volumes:
      - ctakes-overrides:/overrides

  ctakes-covid:
    extends: ctakes-covid-base
    profiles:
      - etl-support
      - etl-support-gpu

  cnlp-transformers:
    image: smartonfhir/cnlp-transformers:negation-0.4-cpu
    profiles:
      - etl-support
    networks:
      - cumulus-etl

  cnlp-transformers-gpu:
    image: smartonfhir/cnlp-transformers:negation-0.4-gpu
    profiles:
      - etl-support-gpu
    networks:
      - cumulus-etl
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  llama2:
    image: ghcr.io/huggingface/text-generation-inference:1.0.0
    environment:
      # If you update anything here that could affect NLP results, consider updating the
      # task_version of any tasks that use this docker.
      - HUGGING_FACE_HUB_TOKEN
      - MAX_BATCH_PREFILL_TOKENS=2048  # default of 4096 overwhelms a 16GB machine
      - MODEL_ID=meta-llama/Llama-2-7b-chat-hf
      - REVISION=08751db2aca9bf2f7f80d2e516117a53d7450235
    healthcheck:
      # There's no curl or wget inside this container, but there is python3!
      test: ["CMD", "python3", "-c", "import socket; socket.create_connection(('localhost', 80))"]
      start_period: 20m  # give plenty of time for startup, since we may be downloading a model
    volumes:
      - hf-data:/data
    networks:
      - cumulus-etl
    ports:
      - 8086:80
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  # these images are intended specifically for development work
  cumulus-etl-test:
    extends:
      service: cumulus-etl-base
    build: 
      context: .
      target: cumulus-etl-test
    environment:
      - URL_CTAKES_REST=http://ctakes-covid-test:8080/ctakes-web-rest/service/analyze
      - URL_CNLP_NEGATION=http://cnlp-transformers-test:8000/negation/process
    volumes: 
      - ./:/cumulus-etl/
    working_dir: /cumulus-etl
    command: 
      - /cumulus-etl/tests/data/simple/ndjson-input 
      - /cumulus-etl/example-output 
      - /cumulus-etl/example-phi-build 
      - --output-format=ndjson
    profiles:
      - test

  cumulus-etl-test-gpu:
    extends:
      service: cumulus-etl-base
    build: 
      context: .
      target: cumulus-etl-test
    environment:
      - URL_CTAKES_REST=http://ctakes-covid-test:8080/ctakes-web-rest/service/analyze
      - URL_CNLP_NEGATION=http://cnlp-transformers-test-gpu:8000/negation/process
    volumes: 
      - ./:/cumulus-etl/
    working_dir: /cumulus-etl
    command: 
      - /cumulus-etl/tests/data/simple/ndjson-input 
      - /cumulus-etl/example-output 
      - /cumulus-etl/example-phi-build 
      - --output-format=ndjson
    profiles:
      - test-gpu


  ctakes-covid-test:
    extends:
      service: ctakes-covid-base
    profiles:
      - test
      - test-gpu
    ports:
      - 8080:8080

  cnlp-transformers-test:
    image: smartonfhir/cnlp-transformers:negation-latest-cpu
    #build: 
    #  context: ../cnlp_transformers/docker
    #  dockerfile: Dockerfile.cpu
    ports:
      - 8000:8000
    profiles:
      - test
    networks:
      - cumulus-etl

  cnlp-transformers-test-gpu:
    image: smartonfhir/cnlp-transformers:negation-latest-gpu
    #build: 
    #  context: ../cnlp_transformers/docker
    #  dockerfile: Dockerfile.gpu
    ports:
      - 8000:8000
    profiles:
      - test-gpu
    networks:
      - cumulus-etl
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

networks:
  cumulus-etl:
    name: cumulus-etl

volumes:
  ctakes-overrides:
  hf-data:
