services:

  # This service is shared by all other services - be careful when adding to it.
  common-base:
    # Set an arbitrary base image, because older versions of Docker (24.x at least; 27.x is fixed)
    # require an image on all services. But every subclass of common-base here redefines
    # the image property. At some point (when platforms have newer Dockers) we can drop this.
    image: alpine:latest
    environment:
      # These are commonly-supported networking environment variables.
      # Yes, the lowercase forms are intentional.
      # See https://about.gitlab.com/blog/2021/01/27/we-need-to-talk-no-proxy/ for background.
      - HTTP_PROXY
      - http_proxy
      - HTTPS_PROXY
      - https_proxy
      - ALL_PROXY
      - all_proxy
      - NO_PROXY
      - no_proxy
      - SSL_CERT_DIR
      - SSL_CERT_FILE
    profiles:
      - base

  cumulus-etl-base:
    extends: common-base
    image: smartonfhir/cumulus-etl:latest
    pull_policy: always  # ensure folks are using the lastest release
    build:
      context: .
      target: cumulus-etl
    environment:
      # Environment variobles to pull in from the host
      - AWS_ACCESS_KEY_ID
      - AWS_DEFAULT_PROFILE
      - AWS_PROFILE
      - AWS_SECRET_ACCESS_KEY
      - AWS_SESSION_TOKEN
      - AZURE_OPENAI_API_KEY
      - AZURE_OPENAI_ENDPOINT
      # Internal environment variobles
      - URL_CTAKES_REST=http://ctakes-covid:8080/ctakes-web-rest/service/analyze
      - URL_CNLP_NEGATION=http://cnlpt-negation:8000/negation/process
      - URL_CNLP_TERM_EXISTS=http://cnlpt-term-exists:8000/termexists/process
    volumes:
      - $HOME/.aws/:/root/.aws/:ro
      - ctakes-overrides:/ctakes-overrides
    networks:
      - cumulus-etl
    profiles:
      - base

  cumulus-etl:
    extends: cumulus-etl-base
    profiles:
      - etl

  cumulus-etl-gpu:
    extends: cumulus-etl-base
    environment:
      - CUMULUS_LLAMA2_URL=http://llama2:8086/v1
      - URL_CNLP_NEGATION=http://cnlpt-negation-gpu:8000/negation/process
      - URL_CNLP_TERM_EXISTS=http://cnlpt-term-exists-gpu:8000/termexists/process
    profiles:
      - etl-gpu

  ctakes-covid-base:
    extends: common-base
    image: smartonfhir/ctakes-covid:1.1.1
    environment:
      - ctakes_umlsuser=umls_api_key
      - ctakes_umlspw=${UMLS_API_KEY:-}
    healthcheck:
      test: echo 'Must set the UMLS_API_KEY environment variable.' && test -n "${UMLS_API_KEY:-}"
      retries: 1
      interval: 1s
    networks:
      - cumulus-etl
    profiles:
      - base
    volumes:
      - ctakes-overrides:/overrides

  ctakes-covid:
    extends: ctakes-covid-base
    profiles:
      - covid-symptom
      - covid-symptom-gpu
      - upload-notes
      - upload-notes-gpu

  cnlpt-negation:
    extends: common-base
    image: smartonfhir/cnlp-transformers:negation-0.6.1-cpu
    profiles:
      - covid-symptom
      - upload-notes
    networks:
      - cumulus-etl

  cnlpt-negation-gpu:
    extends: common-base
    image: smartonfhir/cnlp-transformers:negation-0.6.1-gpu
    platform: linux/amd64
    profiles:
      - covid-symptom-gpu
      - upload-notes-gpu
    networks:
      - cumulus-etl
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  cnlpt-term-exists:
    extends: common-base
    image: smartonfhir/cnlp-transformers:termexists-0.6.1-cpu
    platform: linux/amd64
    profiles:
      - covid-symptom
    networks:
      - cumulus-etl

  cnlpt-term-exists-gpu:
    extends: common-base
    image: smartonfhir/cnlp-transformers:termexists-0.6.1-gpu
    platform: linux/amd64
    profiles:
      - covid-symptom-gpu
    networks:
      - cumulus-etl
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  # This is a WIP llama2 setup, currently suitable for running in a g5.xlarge AWS instance.
  llama2:
    extends: common-base
    image: vllm/vllm-openai:v0.10.0
    environment:
      - HF_TOKEN
      - HUGGING_FACE_HUB_TOKEN
    command:
      # If you update anything here that could affect NLP results, consider updating the
      # task_version of any tasks that use this docker.
      - --download-dir=/data
      - --model=meta-llama/Llama-2-13b-chat-hf
      - --port=8086
      - --quantization=bitsandbytes  # 4bit
      - --revision=a2cb7a712bb6e5e736ca7f8cd98167f81a0b5bd8
    healthcheck:
      test: ["CMD", "wget", "localhost:8086/health", "--output-document=/dev/null"]
      start_period: 20m  # give plenty of time for startup, since we may be downloading a model
    volumes:
      - vllm-data:/data
    profiles:
      - hf-test
    networks:
      - cumulus-etl
    ports:
      - 8086:8086
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  # these images are intended specifically for development work
  cumulus-etl-test:
    extends:
      service: cumulus-etl-base
    build: 
      context: .
      target: cumulus-etl-test
    environment:
      - URL_CTAKES_REST=http://ctakes-covid-test:8080/ctakes-web-rest/service/analyze
      - URL_CNLP_NEGATION=http://cnlp-transformers-test:8000/negation/process
    volumes: 
      - ./:/cumulus-etl/
    working_dir: /cumulus-etl
    command:
      - /cumulus-etl/tests/data/simple/ndjson-input 
      - /cumulus-etl/example-output 
      - /cumulus-etl/example-phi-build 
      - --output-format=ndjson
    profiles:
      - test

  cumulus-etl-test-gpu:
    extends:
      service: cumulus-etl-base
    build: 
      context: .
      target: cumulus-etl-test
    environment:
      - URL_CTAKES_REST=http://ctakes-covid-test:8080/ctakes-web-rest/service/analyze
      - URL_CNLP_NEGATION=http://cnlp-transformers-test-gpu:8000/negation/process
    volumes: 
      - ./:/cumulus-etl/
    working_dir: /cumulus-etl
    command: 
      - /cumulus-etl/tests/data/simple/ndjson-input 
      - /cumulus-etl/example-output 
      - /cumulus-etl/example-phi-build 
      - --output-format=ndjson
    profiles:
      - test-gpu


  ctakes-covid-test:
    extends:
      service: ctakes-covid-base
    profiles:
      - test
      - test-gpu
    ports:
      - 8080:8080

  cnlp-transformers-test:
    extends: common-base
    image: smartonfhir/cnlp-transformers:negation-latest-cpu
    platform: linux/amd64
    #build: 
    #  context: ../cnlp_transformers/docker
    #  dockerfile: Dockerfile.cpu
    ports:
      - 8000:8000
    profiles:
      - test
    networks:
      - cumulus-etl

  cnlp-transformers-test-gpu:
    extends: common-base
    image: smartonfhir/cnlp-transformers:negation-latest-gpu
    platform: linux/amd64
    #build: 
    #  context: ../cnlp_transformers/docker
    #  dockerfile: Dockerfile.gpu
    ports:
      - 8000:8000
    profiles:
      - test-gpu
    networks:
      - cumulus-etl
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

networks:
  cumulus-etl:
    name: cumulus-etl

volumes:
  ctakes-overrides:
  vllm-data:
