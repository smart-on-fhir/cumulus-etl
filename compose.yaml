services:

  # This service is shared by all other services - be careful when adding to it.
  common-base:
    # Set an arbitrary base image, because older versions of Docker (24.x at least; 27.x is fixed)
    # require an image on all services. But every subclass of common-base here redefines
    # the image property. At some point (when platforms have newer Dockers) we can drop this.
    image: alpine:latest
    environment:
      # These are commonly-supported networking environment variables.
      # Yes, the lowercase forms are intentional.
      # See https://about.gitlab.com/blog/2021/01/27/we-need-to-talk-no-proxy/ for background.
      - HTTP_PROXY
      - http_proxy
      - HTTPS_PROXY
      - https_proxy
      - ALL_PROXY
      - all_proxy
      - NO_PROXY
      - no_proxy
      - SSL_CERT_DIR
      - SSL_CERT_FILE
    profiles:
      - base

  cumulus-etl-base:
    extends: common-base
    image: smartonfhir/cumulus-etl:latest
    pull_policy: always  # ensure folks are using the lastest release
    build:
      context: .
      target: cumulus-etl
    environment:
      # Environment variobles to pull in from the host
      - AWS_ACCESS_KEY_ID
      - AWS_ATHENA_WORK_GROUP
      - AWS_DEFAULT_PROFILE
      - AWS_DEFAULT_REGION
      - AWS_PROFILE
      - AWS_REGION
      - AWS_SECRET_ACCESS_KEY
      - AWS_SESSION_TOKEN
      - AZURE_OPENAI_API_KEY
      - AZURE_OPENAI_ENDPOINT
      # Internal environment variobles
      - URL_CTAKES_REST=http://ctakes-covid:8080/ctakes-web-rest/service/analyze
      - URL_CNLP_NEGATION=http://cnlpt-negation:8000/negation/process
      - URL_CNLP_TERM_EXISTS=http://cnlpt-term-exists:8000/termexists/process
    volumes:
      - $HOME/.aws/:/root/.aws/:ro
      - ctakes-overrides:/ctakes-overrides
    networks:
      - cumulus-etl
    profiles:
      - base

  cumulus-etl:
    extends: cumulus-etl-base
    profiles:
      - etl

  cumulus-etl-gpu:
    extends: cumulus-etl-base
    environment:
      - CUMULUS_GPT_OSS_120B_URL=http://gpt-oss-120b:8086/v1
      - CUMULUS_LLAMA4_SCOUT_URL=http://llama4-scout:8087/v1
      - URL_CNLP_NEGATION=http://cnlpt-negation-gpu:8000/negation/process
      - URL_CNLP_TERM_EXISTS=http://cnlpt-term-exists-gpu:8000/termexists/process
    profiles:
      - etl-gpu

  ctakes-covid-base:
    extends: common-base
    image: smartonfhir/ctakes-covid:1.1.1
    environment:
      - ctakes_umlsuser=umls_api_key
      - ctakes_umlspw=${UMLS_API_KEY:-}
    healthcheck:
      test: echo 'Must set the UMLS_API_KEY environment variable.' && test -n "${UMLS_API_KEY:-}"
      retries: 1
      interval: 1s
    networks:
      - cumulus-etl
    profiles:
      - base
    volumes:
      - ctakes-overrides:/overrides

  ctakes-covid:
    extends: ctakes-covid-base
    profiles:
      - covid-symptom
      - covid-symptom-gpu
      - upload-notes
      - upload-notes-gpu

  cnlpt-negation:
    extends: common-base
    image: smartonfhir/cnlp-transformers:negation-0.6.1-cpu
    profiles:
      - covid-symptom
      - upload-notes
    networks:
      - cumulus-etl

  cnlpt-negation-gpu:
    extends: common-base
    image: smartonfhir/cnlp-transformers:negation-0.6.1-gpu
    platform: linux/amd64
    profiles:
      - covid-symptom-gpu
      - upload-notes-gpu
    networks:
      - cumulus-etl
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  cnlpt-term-exists:
    extends: common-base
    image: smartonfhir/cnlp-transformers:termexists-0.6.1-cpu
    platform: linux/amd64
    profiles:
      - covid-symptom
    networks:
      - cumulus-etl

  cnlpt-term-exists-gpu:
    extends: common-base
    image: smartonfhir/cnlp-transformers:termexists-0.6.1-gpu
    platform: linux/amd64
    profiles:
      - covid-symptom-gpu
    networks:
      - cumulus-etl
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  gpt-oss-120b:
    # Needs 80GB of GPU memory. A g6.12xlarge EC2 instance should work.
    # Docs: https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html
    extends: common-base
    image: vllm/vllm-openai:gptoss  # TODO: once stabilized, revert to main releases
    environment:
      - VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1  # needed for NVIDIA A10 and A100 chips
    command:
      - --download-dir=/data
      - --port=8086
      # If you update anything here that could affect NLP results, consider updating the
      # task_version of any tasks that use this docker.
      - --model=openai/gpt-oss-120b
      - --revision=bc75b44b8a2a116a0e4c6659bcd1b7969885f423
      - --tensor-parallel-size=4
    shm_size: 32G
    healthcheck:
      test: ["CMD", "wget", "localhost:8086/health", "--output-document=/dev/null"]
      start_period: 40m  # give plenty of time for startup, loading is slow
    volumes:
      - vllm-data:/data
    profiles:
      - irae-gpu
    networks:
      - cumulus-etl
    ports:
      - 8086:8086
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  llama4-scout:  # WIP, have not gotten it to run successfully yet
    # Needs 80GB of GPU memory. A g6.12xlarge EC2 instance should work.
    # Docs: https://docs.vllm.ai/projects/recipes/en/latest/Llama/Llama4-Scout.html
    extends: common-base
    image: vllm/vllm-openai:v0.10.0
    environment:
      - HF_TOKEN
      - HUGGING_FACE_HUB_TOKEN
      - VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1  # needed for NVIDIA A10 and A100 chips
    command:
      - --download-dir=/data
      - --port=8087
      # If you update anything here that could affect NLP results, consider updating the
      # task_version of any tasks that use this docker.
      - --model=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8
      - --revision=d1cf1e9db03b67e10422f97f38c8b546dec14789
      - --tensor-parallel-size=4
    shm_size: 32G
    healthcheck:
      test: ["CMD", "wget", "localhost:8087/health", "--output-document=/dev/null"]
      start_period: 40m  # give plenty of time for startup, loading is slow
    volumes:
      - vllm-data:/data
    profiles:
      - irae-gpu
    networks:
      - cumulus-etl
    ports:
      - 8087:8087
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  # these images are intended specifically for development work
  cumulus-etl-test:
    extends:
      service: cumulus-etl-base
    build: 
      context: .
      target: cumulus-etl-test
    environment:
      - URL_CTAKES_REST=http://ctakes-covid-test:8080/ctakes-web-rest/service/analyze
      - URL_CNLP_NEGATION=http://cnlp-transformers-test:8000/negation/process
    volumes: 
      - ./:/cumulus-etl/
    working_dir: /cumulus-etl
    command:
      - /cumulus-etl/tests/data/simple/ndjson-input 
      - /cumulus-etl/example-output 
      - /cumulus-etl/example-phi-build 
      - --output-format=ndjson
    profiles:
      - test

  cumulus-etl-test-gpu:
    extends:
      service: cumulus-etl-base
    build: 
      context: .
      target: cumulus-etl-test
    environment:
      - URL_CTAKES_REST=http://ctakes-covid-test:8080/ctakes-web-rest/service/analyze
      - URL_CNLP_NEGATION=http://cnlp-transformers-test-gpu:8000/negation/process
    volumes: 
      - ./:/cumulus-etl/
    working_dir: /cumulus-etl
    command: 
      - /cumulus-etl/tests/data/simple/ndjson-input 
      - /cumulus-etl/example-output 
      - /cumulus-etl/example-phi-build 
      - --output-format=ndjson
    profiles:
      - test-gpu


  ctakes-covid-test:
    extends:
      service: ctakes-covid-base
    profiles:
      - test
      - test-gpu
    ports:
      - 8080:8080

  cnlp-transformers-test:
    extends: common-base
    image: smartonfhir/cnlp-transformers:negation-latest-cpu
    platform: linux/amd64
    #build: 
    #  context: ../cnlp_transformers/docker
    #  dockerfile: Dockerfile.cpu
    ports:
      - 8000:8000
    profiles:
      - test
    networks:
      - cumulus-etl

  cnlp-transformers-test-gpu:
    extends: common-base
    image: smartonfhir/cnlp-transformers:negation-latest-gpu
    platform: linux/amd64
    #build: 
    #  context: ../cnlp_transformers/docker
    #  dockerfile: Dockerfile.gpu
    ports:
      - 8000:8000
    profiles:
      - test-gpu
    networks:
      - cumulus-etl
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

networks:
  cumulus-etl:
    name: cumulus-etl

volumes:
  ctakes-overrides:
  vllm-data:
